{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b396fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yt_dlp import YoutubeDL\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from scraperapi_key import api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde4e77",
   "metadata": {},
   "source": [
    "# Scrape YouTube Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0432fd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 videos.\n"
     ]
    }
   ],
   "source": [
    "search_query = ['aliceinwonderland', 'aliceinwonderlandmovie', 'aliceinwonderland-liveaction', 'alice2010', 'alice in wonderland movie', 'aliceinwonderlandmoviereview','alicereview', 'alice in wonderland','timburton', 'miawasikowska', 'johnnydepp', 'helenabonhancarter']\n",
    "\n",
    "url = f'https://www.youtube.com/results?search_query={search_query}'\n",
    "params = {\n",
    "    'api_key': api_key,\n",
    "    'url': url,\n",
    "    'render': 'true'\n",
    "}\n",
    "\n",
    "video_data = []\n",
    "response = requests.get('https://api.scraperapi.com', params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    videos = soup.find_all('div', {\"id\": \"title-wrapper\", \"class\":\"style-scope ytd-video-renderer\"})\n",
    "    \n",
    "    if videos:\n",
    "        print(f\"Found {len(videos)} videos.\")\n",
    "        temp_video_data = []\n",
    "        for video in videos:\n",
    "            video_details = video.find('a', {'id': 'video-title'})\n",
    "            if video_details:\n",
    "                title = video_details.get('title')\n",
    "                link = video_details['href']\n",
    "                video_url = f\"https://www.youtube.com{link}\"\n",
    "                # Use yt_dlp to get view count\n",
    "                try:\n",
    "                    with YoutubeDL({'quiet': True}) as yt:\n",
    "                        info = yt.extract_info(video_url, download=False)\n",
    "                        views = info.get('view_count', 0)\n",
    "                except Exception:\n",
    "                    views = 0\n",
    "                video_info = {\"title\": title, \"link\": video_url, \"views\": views}\n",
    "                temp_video_data.append(video_info)\n",
    "        # Sort by views descending\n",
    "        video_data = sorted(temp_video_data, key=lambda x: x['views'], reverse=True)\n",
    "        # Save the extracted data to a JSON file\n",
    "        with open(f'videossearch-{search_query[0]}.json', 'w') as json_file:\n",
    "            json.dump(video_data, json_file, indent=4)\n",
    "    else:\n",
    "        print(\"No videos found.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the page:\", response.status_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
